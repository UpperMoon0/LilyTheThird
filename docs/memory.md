# Memory System Documentation

This document describes the long-term memory system, which uses MongoDB and sentence embeddings for storing and retrieving information ("facts").

## Core Components

*   **`memory/mongo_handler.py`**:
    *   Defines the `MongoHandler` class.
    *   Manages the connection to a MongoDB database specified by the `MONGO_URI` environment variable.
    *   Loads a sentence transformer model (`all-MiniLM-L6-v2` by default) using the `sentence-transformers` library for generating vector embeddings.
    *   Interacts with a specific MongoDB collection (default: "memories").
*   **Sentence Transformer Model**:
    *   The `all-MiniLM-L6-v2` model is used to convert text content into numerical vector embeddings. These embeddings capture the semantic meaning of the text.
*   **MongoDB Collection ("memories")**:
    *   Stores documents representing individual pieces of information, referred to as "facts".
    *   Each fact document typically contains:
        *   `_id`: Unique MongoDB ObjectId.
        *   `type`: Set to "fact" to distinguish from other potential document types.
        *   `content`: The textual information of the fact.
        *   `content_embedding`: The numerical vector embedding of the `content`, generated by the sentence transformer model.
        *   `timestamp`: UTC timestamp of when the fact was added.
        *   `metadata`: An optional dictionary for additional context.

## Key Functionality (`MongoHandler` Methods)

*   **Initialization (`__init__`)**:
    *   Loads `MONGO_URI` from `.env`.
    *   Attempts to connect to MongoDB.
    *   Loads the sentence transformer model (`_load_embedding_model`). Logs warnings/errors if connection or model loading fails.
    *   Ensures a text index exists on the `content` field (`_ensure_text_index`) for potential keyword search fallback (though primary retrieval is semantic).
    *   Runs `cleanup_duplicate_facts` on startup.
*   **Connection Check (`is_connected`)**: Returns `True` if connected to MongoDB, `False` otherwise.
*   **Embedding Model Loading (`_load_embedding_model`)**: Loads the `SentenceTransformer` model specified by `EMBEDDING_MODEL_NAME`. Handles errors during loading.
*   **Adding Facts (`add_fact`)**:
    *   Requires MongoDB connection and a loaded embedding model.
    *   Generates an embedding for the provided `content`.
    *   **Similarity Check**: Compares the new embedding's cosine similarity against all existing fact embeddings in the database.
    *   If the similarity score with any existing fact is >= `0.95` (threshold), the insertion is skipped to prevent near-duplicates, and `None` is returned.
    *   If the similarity check passes, inserts a new document into the collection with `type="fact"`, the content, its embedding, timestamp, and any metadata.
    *   Returns the `ObjectId` of the inserted document on success, `None` on failure or if skipped due to similarity.
*   **Retrieving Facts (Semantic Search) (`retrieve_memories_by_similarity`)**:
    *   Requires MongoDB connection and a loaded embedding model.
    *   Generates an embedding for the `query_text`.
    *   Fetches all existing "fact" documents that have embeddings.
    *   **Local Similarity Calculation**: Calculates the cosine similarity between the query embedding and each fetched fact embedding *locally* within the Python script (does not rely on MongoDB Atlas Vector Search).
    *   Sorts the results by similarity score (descending).
    *   Returns a list of the top `limit` facts (default 5), formatted as dictionaries containing the string `_id` and `content`. Returns an empty list on error or if no facts are found.
*   **Updating Facts (`update_fact`)**:
    *   Requires MongoDB connection and a loaded embedding model.
    *   Takes a `memory_id` (string representation of the ObjectId) and `new_content`.
    *   **Delete-Then-Insert**: Attempts to delete the existing fact document matching the `memory_id`.
    *   If deletion is successful, it generates an embedding for the `new_content`.
    *   Inserts a *new* fact document with the `new_content`, new embedding, current timestamp, and empty metadata.
    *   Returns the `ObjectId` of the *newly inserted* document on success.
    *   Returns `None` if the original `memory_id` is invalid, the document to delete is not found, embedding generation fails, or insertion fails.
*   **Duplicate Cleanup (`cleanup_duplicate_facts`)**:
    *   Runs automatically during `MongoHandler` initialization.
    *   Fetches all facts sorted by timestamp (oldest first).
    *   Compares each fact's embedding with all subsequent (newer) facts using cosine similarity.
    *   If similarity >= `0.95` (threshold), the *newer* fact's ID is marked for deletion.
    *   Deletes all marked facts in a single operation. This ensures that for groups of similar facts, only the oldest one is retained.
*   **Text Index (`_ensure_text_index`)**: Ensures a standard MongoDB text index exists on the `content` field. This is primarily for potential future use or fallback keyword searching, as the main retrieval mechanism is semantic similarity via embeddings. Handles potential conflicts with existing text indexes.
*   **Closing Connection (`close_connection`)**: Closes the MongoDB client connection.

## Integration with LLM Workflow

*   **Automatic Retrieval**: `BaseLLMOrchestrator._retrieve_and_add_memory_context` calls `MongoHandler.retrieve_memories_by_similarity` at the start of processing a user message to find relevant context.
*   **Tool Execution**: The `ToolExecutor` maps the `fetch_memory`, `save_memory`, and `update_memory` tool names to the corresponding `MongoHandler` methods (`retrieve_memories_by_similarity`, `add_fact`, `update_fact`).
*   **Final Memory Step**: The LLM can choose to call `save_memory` or `update_memory` via the `ToolExecutor` at the end of the main tool loop based on the conversation context and retrieved facts.
